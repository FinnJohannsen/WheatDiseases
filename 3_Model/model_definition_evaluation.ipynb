{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition and Evaluation\n",
    "## Table of Contents\n",
    "1. [Model Selection](#model-selection)\n",
    "2. [Feature Engineering](#feature-engineering)\n",
    "3. [Hyperparameter Tuning](#hyperparameter-tuning)\n",
    "4. [Implementation](#implementation)\n",
    "5. [Evaluation Metrics](#evaluation-metrics)\n",
    "6. [Comparative Analysis](#comparative-analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# Import models you're considering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "[Discuss the type(s) of models you consider for this task, and justify the selection.]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "[Describe any additional feature engineering you've performed beyond what was done for the baseline model.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Define the paths to the folders containing the image data\n",
    "train_data_dir = '../Dataset/cleaned_scaled_split/train/'\n",
    "validation_data_dir = '../Dataset/cleaned_scaled_split/val/'\n",
    "test_data_dir = '../Dataset/cleaned_scaled_split/test/'\n",
    "\n",
    "# Set the parameters for image data generation\n",
    "batch_size = 64\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "# Load the training data from the folders\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=train_data_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    color_mode='rgb',\n",
    "    batch_size=64,\n",
    "    image_size=(256, 256))\n",
    "\n",
    "\n",
    "# Load the validation data from the folders\n",
    "validation_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=validation_data_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    color_mode='rgb',\n",
    "    batch_size=64,\n",
    "    image_size=(256, 256))\n",
    "\n",
    "# Load the validation data from the folders\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=test_data_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    color_mode='rgb',\n",
    "    batch_size=64,\n",
    "    image_size=(256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "[Discuss any hyperparameter tuning methods you've applied, such as Grid Search or Random Search, and the rationale behind them.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement hyperparameter tuning\n",
    "# Example using GridSearchCV with a DecisionTreeClassifier\n",
    "# param_grid = {'max_depth': [2, 4, 6, 8]}\n",
    "# grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)\n",
    "# grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "[Implement the final model(s) you've selected based on the above steps.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X, Y):\n",
    "    \"\"\"trains a convolutional neural network to classify the dataset\"\"\"\n",
    "    X_p = keras.applications.resnet50.preprocess_input(X)\n",
    "    #Y_p = keras.utils.to_categorical(Y, 10)\n",
    "    return X_p,Y #Y_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(256, 256, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = keras.applications.ResNet50(weights='imagenet',\n",
    "                                 include_top=False, input_tensor=inputs)\n",
    "\n",
    "for layer in resnet.layers[:170]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "#model.add(keras.layers.Lambda(lambda x: tf.image.resize(x, (224, 224))))\n",
    "model.add(resnet)\n",
    "model.add(keras.layers.GlobalAveragePooling2D())\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(256, activation='relu'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(128, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.3))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.3))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(10, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for trainX, trainy in train_ds:\n",
    "#     xtemp, ytemp = preprocess_data(trainX, trainy)\n",
    "#     #if len(xtemp) == 64:\n",
    "#     if len(trainX_list) == 0:\n",
    "#         trainX_list = xtemp\n",
    "#         trainy_list = ytemp\n",
    "#     else:\n",
    "#         trainX_list = np.concatenate((trainX_list, xtemp))\n",
    "#         trainy_list = np.concatenate((trainy_list, ytemp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataprocessing(dataset,listX,listy):\n",
    "    for X, y in dataset:\n",
    "        xtemp, ytemp = preprocess_data(X, y)\n",
    "        if len(listX) == 0:\n",
    "            listX = xtemp\n",
    "            listy = ytemp\n",
    "        else:\n",
    "            listX = np.concatenate((listX, xtemp))\n",
    "            listy = np.concatenate((listy, ytemp))\n",
    "    return listX, listy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_list, trainy_list = dataprocessing(train_ds,np.array([]), np.array([]))\n",
    "valX_list, valy_list = dataprocessing(validation_ds,np.array([]), np.array([]))\n",
    "testX_list, testy_list =  dataprocessing(test_ds,np.array([]), np.array([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(trainX_list, trainy_list, batch_size=64, epochs=10,\n",
    "          validation_data=(valX_list, valy_list), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Res50_no_resize_tvt_split.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "try:\n",
    "    model.summary()\n",
    "except NameError:\n",
    "    model = tf.keras.models.load_model('Res50_no_resize_tvt_split.keras')\t\n",
    "\n",
    "class_correct = [0]*10\n",
    "class_counters = [0]*10\n",
    "for batch in test_ds:\n",
    "    images, labels = batch\n",
    "    images_pp = keras.applications.resnet50.preprocess_input(images)\n",
    "    predictions = model.predict(images_pp)\n",
    "    y_pred_classes = np.argmax(predictions, axis=1)\n",
    "    print(np.mean(y_pred_classes==labels.numpy()))\n",
    "    for i in range(10):\n",
    "        class_correct[i] += np.sum(y_pred_classes[labels.numpy()==i] == i)\n",
    "        class_counters[i] += np.sum(labels.numpy()==i)\n",
    "\n",
    "calss_accuracy = [class_correct[i]/class_counters[i] for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_dir = '../Dataset/train_cleaned_scaled/train/'\n",
    "\n",
    "datadirs = os.listdir(train_dir)\n",
    "print(datadirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the class labels\n",
    "class_labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(8, 4), dpi=150)\n",
    "plt.grid()\n",
    "plt.bar(class_labels, np.asarray(calss_accuracy)*100)\n",
    "\n",
    "# Add labels and title\n",
    "#plt.xlabel('Class')\n",
    "plt.ylabel('Accuracy in %')\n",
    "plt.title('Accuracy of the baseline model on the test data for each class')\n",
    "\n",
    "plt.ylim(60, 100)\n",
    "plt.xticks(class_labels,datadirs, rotation=45, ha='right')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "[Clearly specify which metrics you'll use to evaluate the model performance, and why you've chosen these metrics.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using your chosen metrics\n",
    "# Example for classification\n",
    "# y_pred = model.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Example for regression\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Your evaluation code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis\n",
    "\n",
    "[Compare the performance of your model(s) against the baseline model. Discuss any improvements or setbacks and the reasons behind them.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative Analysis code (if applicable)\n",
    "# Example: comparing accuracy of the baseline model and the new model\n",
    "# print(f\"Baseline Model Accuracy: {baseline_accuracy}, New Model Accuracy: {new_model_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
